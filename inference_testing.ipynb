{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d89aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# test model loading and vocab loading\n",
    "import torch\n",
    "from transformer.transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from data.translation_data import TranslationData\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fcae7",
   "metadata": {},
   "source": [
    "# Test Loading a model, vocabulary and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Data Loaders ready\n",
      "Cuda available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.7900 | BLEU Score: 3.40\n"
     ]
    }
   ],
   "source": [
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load best model\n",
    "model_path = './checkpoints/test_checkpoints_med4070/best_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# get tokenizers form sentence piece\n",
    "sp_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_tokenizer.load('data/spm.model')\n",
    "\n",
    "# load hypter parameters\n",
    "args = checkpoint['args']\n",
    "model = Transformer(vocab_size=sp_tokenizer.get_piece_size(), d_model=args['d_model'], n_heads=args['n_heads'],\n",
    "                       max_len=args['max_len'], dropout_rate = args['dropout_rate'],\n",
    "                       encoding_type='sinusoidal', hidden_ff_d=args['d_model']*4,\n",
    "                       num_encoder_layers=args['num_encoder_layers'],\n",
    "                       num_decoder_layers=args['num_encoder_layers']).to(device=device)\n",
    "\n",
    "# load dataset\n",
    "data_module = TranslationData(batch_size=args['batch_size'],\n",
    "                              max_len=args['max_len'], tokenizer=sp_tokenizer)\n",
    "data_module.prepare_data()\n",
    "# get validation loader\n",
    "_, valid_loader, _ = data_module.get_dataloaders()\n",
    "\n",
    "# create a trainer object for inference\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=data_module.special_tokens['<pad>'])\n",
    "trainer = Trainer(model=model, val_loader=valid_loader, loss_fn=loss_fn, tokenizer=sp_tokenizer)\n",
    "trainer.load_checkpoint(path=model_path)\n",
    "# run validation only\n",
    "val_loss, bleu_score = trainer.validate()\n",
    "print(f\"Val Loss: {val_loss:.04f} | BLEU Score: {bleu_score:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bd31",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc831255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentences: Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.\n",
      "Predicted Translation Greedy: J'ai entendu parler de la taille de la taille de la taille de la taille de la taille de ces deux ans, qui a été faite en train de la taille de ces deux ans, qui a été faite en train de la taille de la taille de ces deux ans.\n",
      "Predicted Translation Beam: J'ai montré qu'il s'il y a deux ans, qui a deux ans, qui a deux ans, ce qui a deux ans, qui a deux ans, ce qui a deux ans à la taille de la taille de la taille de la taille de la taille de ces deux ans.\n",
      "==================================================\n",
      "Source sentences: But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.\n",
      "Predicted Translation Greedy: Mais c'est la glace de cette partie de cette partie de cette partie de cette partie de la glace.\n",
      "Predicted Translation Beam: Mais ça n'est pas le problème de cette partie de la glace.\n",
      "==================================================\n",
      "Source sentences: The arctic ice cap is, in a sense, the beating heart of the global climate system.\n",
      "Predicted Translation Greedy: La glace est une glace mondiale, le système économique est une glace mondiale.\n",
      "Predicted Translation Beam: La glace est un système mondialisation de la glace.\n",
      "==================================================\n",
      "Source sentences: It expands in winter and contracts in summer.\n",
      "Predicted Translation Greedy: Il s'avère que les pédent dans les pédients.\n",
      "Predicted Translation Beam: Il s'avère qu'il s'agit de l'air.\n",
      "==================================================\n",
      "Source sentences: The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.\n",
      "Predicted Translation Greedy: Le deuxième de ce que je vais vous montrer un peu près de 25 ans.\n",
      "Predicted Translation Beam: La dernière, je vais vous montrer ce que vous montrer ce qu'il y a 15 ans.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# take a batch from the validation loader\n",
    "src_batch, tgt_batch = next(iter(valid_loader))\n",
    "\n",
    "# send to device\n",
    "src_batch = src_batch.to(device)\n",
    "src_batch_cpu = src_batch.cpu().tolist()\n",
    "# run inference\n",
    "src_sentences = trainer.decode_ids(id_sequences=src_batch,)\n",
    "translated_sentences = trainer.infer(src=src_batch, type='greedy')\n",
    "translated_sentences_beam = trainer.infer(src=src_batch, type='beam')\n",
    "\n",
    "# Print some translations\n",
    "for idx in range(5):  # first 5 examples+\n",
    "    print(f'Source sentences: {src_sentences[idx]}')\n",
    "    print(f\"Predicted Translation Greedy: {translated_sentences[idx]}\")\n",
    "    print(f\"Predicted Translation Beam: {translated_sentences_beam[idx]}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d2732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
