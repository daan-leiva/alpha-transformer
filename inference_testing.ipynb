{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d89aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# test model loading and vocab loading\n",
    "import torch\n",
    "from transformer.transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from data.translation_data import TranslationData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fcae7",
   "metadata": {},
   "source": [
    "# Test Loading a model, vocabulary and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78dc211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocabularies from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 232825/232825 [00:20<00:00, 11542.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes: src = 10000,target = 10000\n",
      "Data Loaders ready\n",
      "Cuda available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.6319 | BLEU Score: 3.96\n"
     ]
    }
   ],
   "source": [
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load best model\n",
    "model_path = './checkpoints/test_checkpoints_small/best_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# load vocabs\n",
    "src_vocab = checkpoint['src_vocab']\n",
    "target_vocab = checkpoint['target_vocab']\n",
    "pad_id = src_vocab['<pad>']\n",
    "\n",
    "# load hypter parameters\n",
    "args = checkpoint['args']\n",
    "model = Transformer(vocab_size=args['vocab_size'], d_model=args['d_model'], n_heads=args['n_heads'],\n",
    "                       max_len=args['max_len'], dropout_rate = args['dropout_rate'],\n",
    "                       encoding_type='sinusoidal', hidden_ff_d=args['d_model']*4,\n",
    "                       num_encoder_layers=args['num_encoder_layers'],\n",
    "                       num_decoder_layers=args['num_encoder_layers']).to(device=device)\n",
    "\n",
    "# load dataset\n",
    "data_module = TranslationData(batch_size=args['batch_size'],\n",
    "                              src_vocab=src_vocab, max_vocab_size=args['vocab_size'],\n",
    "                              max_len=args['max_len'])\n",
    "data_module.prepare_data()\n",
    "# get validation loader\n",
    "_, valid_loader, _ = data_module.get_dataloaders()\n",
    "\n",
    "# create a trainer object for inference\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "trainer = Trainer(model=model, val_loader=valid_loader, loss_fn=loss_fn)\n",
    "trainer.load_checkpoint(path=model_path)\n",
    "# run validation only\n",
    "val_loss, bleu_score = trainer.validate()\n",
    "print(f\"Val Loss: {val_loss:.04f} | BLEU Score: {bleu_score:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bd31",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc831255",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trainer.create_src_mask() missing 1 required positional argument: 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# run inference\u001b[39;00m\n\u001b[1;32m      8\u001b[0m src_sentences \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mdecode_ids(id_sequences\u001b[38;5;241m=\u001b[39msrc_batch, id2word\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mid2word_src)\n\u001b[0;32m----> 9\u001b[0m translated_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid2word_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgreedy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m translated_sentences_beam \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39minfer(src\u001b[38;5;241m=\u001b[39msrc_batch, id2word\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mid2word_target, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Print some translations\u001b[39;00m\n",
      "File \u001b[0;32m~/source/alpha_transformer/trainer.py:343\u001b[0m, in \u001b[0;36mTrainer.infer\u001b[0;34m(self, src, id2word, type)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 343\u001b[0m     generated_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    345\u001b[0m     generated_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam_search_decode(src)\n",
      "File \u001b[0;32m~/source/alpha_transformer/trainer.py:436\u001b[0m, in \u001b[0;36mTrainer.greedy_decode\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    433\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_len\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# creta the src maks\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_src_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# create a variable for the past key values to reuse\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# will be none for the first token\u001b[39;00m\n\u001b[1;32m    440\u001b[0m past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.create_src_mask() missing 1 required positional argument: 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "# take a batch from the validation loader\n",
    "src_batch, target_batch = next(iter(valid_loader))\n",
    "\n",
    "# send to device\n",
    "src_batch = src_batch.to(device)\n",
    "src_batch_cpu = src_batch.cpu().tolist()\n",
    "# run inference\n",
    "src_sentences = trainer.decode_ids(id_sequences=src_batch, id2word=trainer.id2word_src)\n",
    "translated_sentences = trainer.infer(src=src_batch, id2word=trainer.id2word_target, type='greedy')\n",
    "translated_sentences_beam = trainer.infer(src=src_batch, id2word=trainer.id2word_target, type='beam')\n",
    "\n",
    "# Print some translations\n",
    "for idx in range(5):  # first 5 examples+\n",
    "    print(f'Source sentences: {src_sentences[idx]}')\n",
    "    print(f\"Predicted Translation Greedy: {translated_sentences[idx]}\")\n",
    "    print(f\"Predicted Translation Beam: {translated_sentences_beam[idx]}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660450d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
