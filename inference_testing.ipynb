{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d89aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# test model loading and vocab loading\n",
    "import torch\n",
    "from transformer.transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from data.translation_data import TranslationData\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fcae7",
   "metadata": {},
   "source": [
    "# Test Loading a model, vocabulary and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP piece size ('vocab size'): 32000\n",
      "Loading dataset...\n",
      "Data Loaders ready\n",
      "Cuda available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.9293 | BLEU Score: 5.10\n"
     ]
    }
   ],
   "source": [
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load best model\n",
    "model_path = './checkpoints/small_model_de3/best_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# load hypter parameters\n",
    "args = checkpoint['args']\n",
    "\n",
    "# get tokenizers form sentence piece\n",
    "sp_tokenizer = spm.SentencePieceProcessor()\n",
    "if args['tgt_lang'] == 'fr':\n",
    "    sp_tokenizer.load('data/spm_en_fr.model')\n",
    "elif args['tgt_lang'] == 'de':\n",
    "    sp_tokenizer.load('data/spm_en_de.model')\n",
    "else:\n",
    "    raise ValueError(\"only fr and de target languages supported\")\n",
    "\n",
    "print(f\"SP piece size ('vocab size'): {sp_tokenizer.get_piece_size()}\")\n",
    "model = Transformer(vocab_size=sp_tokenizer.get_piece_size(), d_model=args['d_model'], n_heads=args['n_heads'],\n",
    "                       max_len=args['max_len'], dropout_rate = args['dropout_rate'],\n",
    "                       encoding_type='sinusoidal', hidden_ff_d=args['d_model']*4,\n",
    "                       num_encoder_layers=args['num_encoder_layers'],\n",
    "                       num_decoder_layers=args['num_encoder_layers']).to(device=device)\n",
    "\n",
    "# load dataset\n",
    "data_module = TranslationData(src_lang='en', tgt_lang='de', batch_size=args['batch_size'],\n",
    "                              max_len=args['max_len'], tokenizer=sp_tokenizer, small_subset=True)\n",
    "data_module.prepare_data()\n",
    "# get validation loader\n",
    "_, valid_loader, _ = data_module.get_dataloaders()\n",
    "\n",
    "# create a trainer object for inference\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=data_module.special_tokens['<pad>'])\n",
    "trainer = Trainer(model=model, val_loader=valid_loader, loss_fn=loss_fn, tokenizer=sp_tokenizer)\n",
    "trainer.load_checkpoint(path=model_path)\n",
    "# run validation only\n",
    "val_loss, bleu_score = trainer.validate()\n",
    "print(f\"Val Loss: {val_loss:.04f} | BLEU Score: {bleu_score:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bd31",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc831255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentences: Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.\n",
      "Predicted Translation Greedy: L'année dernière fois que j'ai été prise de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille de la taille\n",
      "Predicted Translation Beam: L'année dernière fois que j'ai eu l'année dernière fois que ces deux millions d'année dernière fois que les deux millions de l'année dernière fois que les deux millions d'année dernière fois que les deux millions d'année\n",
      "==================================================\n",
      "Source sentences: But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.\n",
      "Predicted Translation Greedy: Mais ce problème de la glace, parce que ce problème ne se passe pas la glace.\n",
      "Predicted Translation Beam: Mais ce problème de la glace, parce que la glace n'est pas le problème de la glace.\n",
      "==================================================\n",
      "Source sentences: The arctic ice cap is, in a sense, the beating heart of the global climate system.\n",
      "Predicted Translation Greedy: Le système solaire est une symétrie, dans le système solaire, la glace, la glace mondiale.\n",
      "Predicted Translation Beam: La glace, la glace, la glace, la glace est la glace mondiale.\n",
      "==================================================\n",
      "Source sentences: It expands in winter and contracts in summer.\n",
      "Predicted Translation Greedy: C'est vrai, et en Angleterre.\n",
      "Predicted Translation Beam: Il s'est déroulé dans la classe.\n",
      "==================================================\n",
      "Source sentences: The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.\n",
      "Predicted Translation Greedy: La diapo que je vais vous montrer un point de vue de vue de vue de vue de la prochaine diapo.\n",
      "Predicted Translation Beam: L'heure actuelle, je vais vous montrer ce qu'il y a 25 ans.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# take a batch from the validation loader\n",
    "src_batch, tgt_batch = next(iter(valid_loader))\n",
    "\n",
    "# send to device\n",
    "src_batch = src_batch.to(device)\n",
    "src_batch_cpu = src_batch.cpu().tolist()\n",
    "# run inference\n",
    "src_sentences = trainer.decode_ids(id_sequences=src_batch,)\n",
    "translated_sentences = trainer.infer(src=src_batch, type='greedy')\n",
    "translated_sentences_beam = trainer.infer(src=src_batch, type='beam')\n",
    "\n",
    "# Print some translations\n",
    "for idx in range(5):  # first 5 examples+\n",
    "    print(f'Source sentences: {src_sentences[idx]}')\n",
    "    print(f\"Predicted Translation Greedy: {translated_sentences[idx]}\")\n",
    "    print(f\"Predicted Translation Beam: {translated_sentences_beam[idx]}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d2732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
