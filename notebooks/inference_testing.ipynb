{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# test model loading and vocab loading\n",
    "import torch\n",
    "from transformer.transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from data.translation_data import TranslationData\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fcae7",
   "metadata": {},
   "source": [
    "# Test Loading a model, vocabulary and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load best model\n",
    "model_path = './checkpoints/en_fr_large_512_long/best_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# load hypter parameters\n",
    "args = checkpoint['args']\n",
    "\n",
    "# get tokenizers form sentence piece\n",
    "sp_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_tokenizer.load(args['sp_model_path'])\n",
    "\n",
    "print(f\"SP piece size ('vocab size'): {sp_tokenizer.get_piece_size()}\")\n",
    "model = Transformer(vocab_size=sp_tokenizer.get_piece_size(), d_model=args['d_model'], n_heads=args['n_heads'],\n",
    "                       max_len=args['max_len'], dropout_rate = args['dropout_rate'],\n",
    "                       hidden_ff_d=args['d_model']*4,\n",
    "                       num_encoder_layers=args['num_layers'],\n",
    "                       num_decoder_layers=args['num_layers'], encoding_type=args['encoding_type']).to(device=device)\n",
    "\n",
    "# load dataset\n",
    "data_module = TranslationData(src_lang='en', tgt_lang='fr', batch_size=args['batch_size'],\n",
    "                              max_len=args['max_len'], tokenizer=sp_tokenizer)\n",
    "data_module.prepare_data()\n",
    "# get validation loader\n",
    "_, valid_loader, _ = data_module.get_dataloaders()\n",
    "\n",
    "# create a trainer object for inference\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=data_module.special_tokens['<pad>'])\n",
    "trainer = Trainer(model=model, val_loader=valid_loader, loss_fn=loss_fn, tokenizer=sp_tokenizer)\n",
    "trainer.load_checkpoint(path=model_path)\n",
    "# run validation only\n",
    "val_loss, bleu_score = trainer.validate()\n",
    "print(f\"Val Loss: {val_loss:.04f} | BLEU Score: {bleu_score:.02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import os\n",
    "\n",
    "# --- Preparation ---\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy input tensors (match your real shape and vocab size)\n",
    "batch_size = args['batch_size']\n",
    "max_len = args['max_len']\n",
    "vocab_size = 16000\n",
    "pad_token_id = data_module.special_tokens['<pad>']\n",
    "\n",
    "src = torch.randint(0, vocab_size, (batch_size, max_len), dtype=torch.long).to(device)\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, max_len), dtype=torch.long).to(device)\n",
    "\n",
    "# Create masks (must be tensors and on same device)\n",
    "src_mask = trainer.create_src_mask(src, pad_token_id=pad_token_id).to(device)\n",
    "tgt_mask = trainer.create_tgt_mask(tgt, pad_token_id=pad_token_id).to(device)\n",
    "\n",
    "# --- Tracing and Export ---\n",
    "# TorchScript trace\n",
    "traced_model = torch.jit.trace(model, (src, tgt, src_mask, tgt_mask))\n",
    "\n",
    "# Save TorchScript model (optional)\n",
    "traced_model.save(\"transformer_script.pt\")\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    traced_model,\n",
    "    (src, tgt, src_mask, tgt_mask),\n",
    "    \"transformer.onnx\",\n",
    "    input_names=[\"src\", \"tgt\", \"src_mask\", \"tgt_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"src\": {1: \"src_len\"},\n",
    "        \"tgt\": {1: \"tgt_len\"},\n",
    "        \"logits\": {1: \"tgt_len\"}  # logits: (batch, tgt_len, vocab)\n",
    "    },\n",
    "    dynamo=True\n",
    ")\n",
    "print(\"ONNX export completed: transformer.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bd31",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc831255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a batch from the validation loader\n",
    "#src_batch, tgt_batch = next(iter(valid_loader))\n",
    "\n",
    "# send to device\n",
    "#src_batch = src_batch.to(device)\n",
    "# run inference\n",
    "#src_sentences = trainer.decode_ids(id_sequences=src_batch,)\n",
    "from scripts.infer import translate_sentences_non_batched\n",
    "translated_sentences_beam, in_tokens, out_tokens, attention = translate_sentences_non_batched(trainer, sentences=['I like to eat pizza'], decode_type='beam', beam_size=3,return_attention=True )\n",
    "\n",
    "print(len(out_tokens[0]))\n",
    "print(out_tokens)\n",
    "\n",
    "# Print some translations\n",
    "if False:\n",
    "    for idx in range(5):  # first 5 examples+\n",
    "        print(f'Source sentences: {src_sentences[idx]}')\n",
    "        print(f\"Predicted Translation Greedy: {translated_sentences[idx]}\")\n",
    "        print(f\"Predicted Translation Beam: {translated_sentences_beam[idx]}\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(translated_sentences_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb107095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
