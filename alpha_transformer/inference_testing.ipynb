{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d89aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# test model loading and vocab loading\n",
    "import torch\n",
    "from transformer.transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from trainer import Trainer\n",
    "from data.translation_data import TranslationData\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fcae7",
   "metadata": {},
   "source": [
    "# Test Loading a model, vocabulary and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c78dc211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP piece size ('vocab size'): 16000\n",
      "Loading dataset...\n",
      "Small subset mode enabled for faster training\n",
      "Data num_workers: 4\n",
      "Data Loaders ready\n",
      "Cuda available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.0094 | BLEU Score: 23.64\n"
     ]
    }
   ],
   "source": [
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load best model\n",
    "model_path = './checkpoints/en_fr_large_512_long/best_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# load hypter parameters\n",
    "args = checkpoint['args']\n",
    "\n",
    "# get tokenizers form sentence piece\n",
    "sp_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_tokenizer.load(args['sp_model_path'])\n",
    "\n",
    "print(f\"SP piece size ('vocab size'): {sp_tokenizer.get_piece_size()}\")\n",
    "model = Transformer(vocab_size=sp_tokenizer.get_piece_size(), d_model=args['d_model'], n_heads=args['n_heads'],\n",
    "                       max_len=args['max_len'], dropout_rate = args['dropout_rate'],\n",
    "                       hidden_ff_d=args['d_model']*4,\n",
    "                       num_encoder_layers=args['num_layers'],\n",
    "                       num_decoder_layers=args['num_layers'], encoding_type=args['encoding_type']).to(device=device)\n",
    "\n",
    "# load dataset\n",
    "data_module = TranslationData(src_lang='en', tgt_lang='fr', batch_size=args['batch_size'],\n",
    "                              max_len=args['max_len'], tokenizer=sp_tokenizer, small_subset=True)\n",
    "data_module.prepare_data()\n",
    "# get validation loader\n",
    "_, valid_loader, _ = data_module.get_dataloaders()\n",
    "\n",
    "# create a trainer object for inference\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=data_module.special_tokens['<pad>'])\n",
    "trainer = Trainer(model=model, val_loader=valid_loader, loss_fn=loss_fn, tokenizer=sp_tokenizer)\n",
    "trainer.load_checkpoint(path=model_path)\n",
    "# run validation only\n",
    "val_loss, bleu_score = trainer.validate()\n",
    "print(f\"Val Loss: {val_loss:.04f} | BLEU Score: {bleu_score:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bd31",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc831255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src shape:  torch.Size([1, 1, 1, 5])\n",
      "Cross attention shape:  torch.Size([6, 3, 8, 1, 5])\n",
      "Final step count:  21\n",
      "Raw beam log probs: tensor([[-48.9237, -49.4311, -49.5963]], device='cuda:0')\n",
      "Sequence lengths: tensor([[16., 17., 15.]], device='cuda:0')\n",
      "Normalized scores: tensor([[-0.1911, -0.1710, -0.2204]], device='cuda:0')\n",
      "Attention shape before going in:  22 ,  6 ,  torch.Size([3, 8, 1, 5])\n",
      "Stacked dimension:  6 ,  torch.Size([3, 8, 22, 5])\n",
      "final_attn.shape = torch.Size([1, 6, 8, 22, 5])\n",
      "17\n",
      "[['<s>', '▁Je', '▁veux', '▁un', '▁chien', '▁brû', 'le', '▁chaud', ',', '▁je', '▁veux', '▁une', '▁chaude', '▁hot', '▁j', '▁quand', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# take a batch from the validation loader\n",
    "#src_batch, tgt_batch = next(iter(valid_loader))\n",
    "\n",
    "# send to device\n",
    "#src_batch = src_batch.to(device)\n",
    "# run inference\n",
    "#src_sentences = trainer.decode_ids(id_sequences=src_batch,)\n",
    "from scripts.infer import translate_sentences_non_batched\n",
    "translated_sentences_beam, in_tokens, out_tokens, attention = translate_sentences_non_batched(trainer, sentences=['I want a hot dog'], decode_type='beam', beam_size=3,return_attention=True )\n",
    "\n",
    "print(len(out_tokens[0]))\n",
    "print(out_tokens)\n",
    "\n",
    "# Print some translations\n",
    "if False:\n",
    "    for idx in range(5):  # first 5 examples+\n",
    "        print(f'Source sentences: {src_sentences[idx]}')\n",
    "        print(f\"Predicted Translation Greedy: {translated_sentences[idx]}\")\n",
    "        print(f\"Predicted Translation Beam: {translated_sentences_beam[idx]}\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d2732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb107095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
